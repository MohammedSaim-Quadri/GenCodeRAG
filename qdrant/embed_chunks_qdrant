from sentence_transformers import SentenceTransformer
import json
from pathlib import Path
from qdrant_client import QdrantClient
from qdrant_client.models import (
    VectorParams,
    Distance,
    CollectionStatus,
    CollectionConfig,
    PayloadSchemaType,
    PointStruct
)
import uuid
from dotenv import load_dotenv
import os
from tqdm import tqdm
import time

# === Load model ===
print("üß† Loading embedding model...")
model = SentenceTransformer("all-MiniLM-L6-v2")

# === Load code chunks ===
CHUNK_FILE = Path("data/chunks/github_code_chunks.jsonl")
print("üì¶ Reading code chunks...")
chunks = [json.loads(line) for line in open(CHUNK_FILE, "r", encoding="utf-8")]
texts = [chunk["code"] for chunk in chunks]

# === Compute embeddings ===
print(f"üî¢ Computing embeddings for {len(texts)} chunks...")
embeddings = model.encode(texts, batch_size=32, convert_to_numpy=True, show_progress_bar=True)

# === Qdrant Setup ===
load_dotenv()
QDRANT_HOST = os.getenv("QDRANT_HOST", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = "code_chunks"

client = QdrantClient(
    url=QDRANT_HOST, 
    api_key=QDRANT_API_KEY,
    timeout=120
)

# Create collection if it doesn't exist
if not client.collection_exists(collection_name=COLLECTION_NAME):
    client.create_collection(
    collection_name=COLLECTION_NAME,
    vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.COSINE),
    )

    # Create payload indexes for filtering (after collection creation)
    print("üîß Creating payload indexes...")
    try:
        client.create_payload_index(
            collection_name=COLLECTION_NAME,
            field_name="language",
            field_type="keyword"
        )
        print("‚úÖ Created index for 'language' field")
        
        # Optional: Create other indexes if needed
        client.create_payload_index(
            collection_name=COLLECTION_NAME,
            field_name="repo",
            field_type="text"
        )
        print("‚úÖ Created index for 'repo' field")
        
        client.create_payload_index(
            collection_name=COLLECTION_NAME,
            field_name="path",
            field_type="text"
        )
        print("‚úÖ Created index for 'path' field")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not create indexes: {e}")
        print("   You may need to create them manually later")

# === Upload to Qdrant ===
print(f"üì§ Uploading {len(embeddings)} vectors to Qdrant...")

points = [
    PointStruct(
        id=str(uuid.uuid4()),
        vector=embeddings[i],
        payload=chunks[i]
    )
    for i in range(len(embeddings))
]

def upload_batch_with_retry(client, collection_name, batch, max_retries=3):
    """Upload a batch with retry logic"""
    for attempt in range(max_retries):
        try:
            client.upsert(collection_name=collection_name, points=batch)
            return True
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"‚ö†Ô∏è  Retry {attempt + 1}/{max_retries} after {wait_time}s due to: {str(e)[:100]}...")
                time.sleep(wait_time)
            else:
                print(f"‚ùå Failed after {max_retries} attempts: {e}")
                return False
    return False

batch_size = 50  # Reduced batch size
successful_uploads = 0

for i in tqdm(range(0, len(points), batch_size), desc="üì§ Uploading to Qdrant"):
    batch = points[i:i+batch_size]
    if upload_batch_with_retry(client, COLLECTION_NAME, batch):
        successful_uploads += len(batch)
    else:
        print(f"‚ùå Failed to upload batch {i//batch_size + 1}")

print(f"\n‚úÖ Successfully uploaded {successful_uploads}/{len(points)} vectors")

try:
    info = client.get_collection(collection_name=COLLECTION_NAME)
    print(f"üìä Total vectors in collection: {info.points_count}")
except Exception as e:
    print(f"‚ùå Could not confirm upload: {e}")
